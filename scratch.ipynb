{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "from pprint import pprint\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "from cineplex.db import get_db\n",
    "from cineplex.config import Settings\n",
    "from cineplex.logger import Logger\n",
    "\n",
    "settings = Settings()\n",
    "logger = Logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "ray.shutdown()\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cineplex.youtube_channels import (\n",
    "    get_channels_from_youtube,\n",
    "    get_channel_from_db,\n",
    "    get_channels_from_db,\n",
    "    save_channel,\n",
    "    save_channels\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discover Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(settings.youtube_videos_dir)\n",
    "print(f'found {len(files)} files in {settings.youtube_videos_dir}')\n",
    "with open('data/files.json', 'w') as outfile:\n",
    "    json.dump(files, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dedupe Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_idx = {}\n",
    "\n",
    "with open('data/file_list_videos.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    print(len(data))\n",
    "    for file in data:\n",
    "        filename, ext = os.path.splitext(file)\n",
    "        if filename in filename_idx:\n",
    "            filename_idx[filename].append(ext)\n",
    "        else:\n",
    "            filename_idx[filename] = [ext]\n",
    "\n",
    "dupes = []\n",
    "for filename, extensions in filename_idx.items():\n",
    "    if len(extensions) > 1:\n",
    "        dupes.append(filename)\n",
    "\n",
    "remove = []\n",
    "print(f'found {len(dupes)} duplicate filenames')\n",
    "for dup in dupes:\n",
    "    sizes = {}\n",
    "    for ext in filename_idx[dup]:\n",
    "        size = os.path.getsize(os.path.join(settings.youtube_videos_dir, f'{dup}{ext}'))\n",
    "        sizes[ext] = size\n",
    "\n",
    "    # if len(sizes) > 2:\n",
    "    #     print(f'{dup} has {len(sizes)} copies: {sizes}')\n",
    "\n",
    "    smallest = min(sizes, key=sizes.get)\n",
    "\n",
    "    # print(f'Keeping: {dup}{smallest} @ {sizes[smallest]}')\n",
    "    sizes.pop(smallest)\n",
    "    for ext in sizes:\n",
    "        # print(f'Removing: {dup}{ext} @ {sizes[ext]}')\n",
    "        remove.append(f'{dup}{ext}')\n",
    "\n",
    "print(f'found {len(remove)} files to remove')\n",
    "\n",
    "for file in remove:\n",
    "    os.remove(os.path.join(settings.youtube_videos_dir, file))\n",
    "    data.remove(file)\n",
    "\n",
    "print(len(data))\n",
    "\n",
    "# write the json file\n",
    "with open('data/file_list_videos_deduped.json', 'w') as outfile:\n",
    "    json.dump(data, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragments = set()\n",
    "\n",
    "file_list_videos_clean = []\n",
    "\n",
    "with open(os.path.join(settings.data_dir, 'file_list_videos_deduped.json')) as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "    for file in data:\n",
    "        filename, ext = os.path.splitext(file)\n",
    "\n",
    "        # extract the youtube id from the filename\n",
    "        id = filename[-12:]\n",
    "\n",
    "        # handle fragments\n",
    "        if '.' in id:\n",
    "            filename, ext = os.path.splitext(filename)\n",
    "            id = filename[-11:]\n",
    "            fragments.add(id)\n",
    "\n",
    "        elif id[0] != '-':\n",
    "            print(f'{id}|{filename}')\n",
    "\n",
    "        else:\n",
    "            file_list_videos_clean.append(file)\n",
    "\n",
    "print(f'found {len(fragments)} fragments')\n",
    "print(fragments)\n",
    "\n",
    "with open('data/video_fragments.json', 'w') as outfile:\n",
    "    json.dump(list(fragments), outfile)\n",
    "\n",
    "with open('data/file_list_videos_clean.json', 'w') as outfile:\n",
    "    json.dump(file_list_videos_clean, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video File Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_file_index = {}\n",
    "\n",
    "with open(os.path.join(settings.data_dir, 'file_list_videos_clean.json')) as json_file:\n",
    "    data = json.load(json_file)\n",
    "    for file in data:\n",
    "        filename, ext = os.path.splitext(file)\n",
    "        id = filename[-11:]\n",
    "        video_file_index[id] = {'id': id, 'filename': file}\n",
    "with open(os.path.join(settings.data_dir, 'file_index_videos.json'), 'w') as outfile:\n",
    "    json.dump(video_file_index, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(settings.data_dir, 'file_index_videos.json')) as json_file:\n",
    "    video_file_index = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thumbnail File Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thumbnail_file_index = {}\n",
    "\n",
    "with open(os.path.join(settings.data_dir, 'file_list_thumbnails.json')) as json_file:\n",
    "    data = json.load(json_file)\n",
    "    for file in data:\n",
    "        filename, ext = os.path.splitext(file)\n",
    "        id = filename[-11:]\n",
    "        thumbnail_file_index[id] = {'id': id, 'filename': file}\n",
    "with open(os.path.join(settings.data_dir, 'file_index_thumbnails.json'), 'w') as outfile:\n",
    "    json.dump(thumbnail_file_index, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(settings.data_dir, 'file_index_thumbnails.json')) as json_file:\n",
    "    thumbnail_file_index = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata File Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_file_index = {}\n",
    "\n",
    "with open(os.path.join(settings.data_dir, 'file_list_metadata.json')) as json_file:\n",
    "    data = json.load(json_file)\n",
    "    for file in data:\n",
    "        # metadata files have two extensions\n",
    "        filename, ext = os.path.splitext(file)\n",
    "        filename, ext = os.path.splitext(filename)\n",
    "        id = filename[-11:]\n",
    "        metadata_file_index[id] = {'id': id, 'filename': file}\n",
    "with open(os.path.join(settings.data_dir, 'file_index_metadata.json'), 'w') as outfile:\n",
    "    json.dump(metadata_file_index, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(settings.data_dir, 'file_index_metadata.json')) as json_file:\n",
    "    metadata_file_index = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Thumbnails and Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_thumbnails = []\n",
    "missing_metadata = []\n",
    "\n",
    "for id in video_file_index.keys():\n",
    "    if id not in thumbnail_file_index:\n",
    "        missing_thumbnails.append(id)\n",
    "    if id not in metadata_file_index:\n",
    "        missing_metadata.append(id)\n",
    "\n",
    "print(f'found {len(missing_thumbnails)} missing thumbnails')\n",
    "with open(os.path.join(settings.data_dir, 'missing_thumbnails.json'), 'w') as outfile:\n",
    "    json.dump(missing_thumbnails, outfile)\n",
    "\n",
    "print(f'found {len(missing_metadata)} missing metadata')\n",
    "with open(os.path.join(settings.data_dir, 'missing_metadata.json'), 'w') as outfile:\n",
    "    json.dump(missing_metadata, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Metadata Into DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def extract_metadata(input):\n",
    "    id, filename = input\n",
    "\n",
    "    with open(filename, 'r') as f:\n",
    "        \n",
    "        try:\n",
    "            data = json.load(f)\n",
    "        except Exception as e:\n",
    "            logger.error(f'Failed to load metadata: {filename}: {e}')\n",
    "            return id\n",
    "\n",
    "        res = get_db().get(f'video#{id}')\n",
    "        if res:\n",
    "            # print(f'found {id} in db')\n",
    "            return None\n",
    "\n",
    "        doc = {\n",
    "            'id': id,\n",
    "            'title': data['title'] if 'title' in data else id,\n",
    "            'description': data['description'] if 'description' in data else '',\n",
    "            'tags': data['tags'] if 'tags' in data else [],\n",
    "            'categories': data['categories'] if 'categories' in data else [],\n",
    "            'channel_id': data['channel_id'] if 'channel_id' in data else '',\n",
    "            'uploader': data['uploader'] if 'uploader' in data else '',\n",
    "            'upload_date': data['upload_date'] if 'upload_date' in data else '',\n",
    "            'duration': data['duration'] if 'duration' in data else 0,\n",
    "            'view_count': data['view_count'] if 'view_count' in data else 0,\n",
    "            'like_count': data['like_count'] if 'like_count' in data else 0,\n",
    "            'dislike_count': data['dislike_count'] if 'dislike_count' in data else 0,\n",
    "            'average_rating': data['average_rating'] if 'average_rating' in data else 0,\n",
    "            'width': data['width'] if 'width' in data else 0,\n",
    "            'height': data['height']    if 'height' in data else 0,\n",
    "            'format': data['format'] if 'format' in data else '',\n",
    "            'format_id': data['format_id']  if 'format_id' in data else '',\n",
    "            'video_file': video_file_index[id]['filename'] if id in video_file_index else '',\n",
    "            'thumbnail_file': thumbnail_file_index[id]['filename'] if id in thumbnail_file_index else '',\n",
    "            'metadata_file': metadata_file_index[id]['filename'] if id in metadata_file_index else '',\n",
    "        }\n",
    "        get_db().set(f'video#{id}', json.dumps(doc))\n",
    "    \n",
    "    return None\n",
    "\n",
    "filenames = []\n",
    "video_ids = list(video_file_index.keys())\n",
    "# for id in tqdm(video_ids[:200]):\n",
    "for id in tqdm(video_ids):\n",
    "    metadata = metadata_file_index[id]\n",
    "    filename = os.path.join(settings.youtube_metadata_dir, metadata['filename'])\n",
    "    filenames.append((id, filename))\n",
    "\n",
    "bad_metadata = [extract_metadata.remote(x) for x in tqdm(filenames)]\n",
    "bad_metadata = [x for x in tqdm(ray.get(bad_metadata)) if x]\n",
    "\n",
    "with open(os.path.join(settings.data_dir, 'bad_metadata.json'), 'w') as outfile:\n",
    "    json.dump(bad_metadata, outfile)\n",
    "\n",
    "print(f'found {len(bad_metadata)} bad metadata')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move Files to Channel Dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def move_file(src, dst):\n",
    "    try:\n",
    "        # print(f'Moving {src} to {dst}')\n",
    "        if not os.path.exists(dst):\n",
    "            os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
    "            shutil.move(src, dst)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f'Failed to move {src} to {dst}: {e}')\n",
    "        return False\n",
    "\n",
    "@ray.remote\n",
    "def move_files(id):\n",
    "\n",
    "    info = get_db().get(f'video#{id}')\n",
    "    if not info:\n",
    "        logger.error(f'Failed to find {id} in db')\n",
    "        return id\n",
    "    info = json.loads(info)\n",
    "\n",
    "    src_video_file = os.path.join(settings.youtube_videos_dir, info['video_file'])\n",
    "    src_thumbnail_file = os.path.join(settings.youtube_thumbnails_dir, info['thumbnail_file'])\n",
    "    src_metadata_file = os.path.join(settings.youtube_metadata_dir, info['metadata_file'])\n",
    "\n",
    "    channel_id = info['channel_id']\n",
    "    if not channel_id:\n",
    "        channel_id = '__unknown__'\n",
    "        \n",
    "    dst_video_file = os.path.join(settings.youtube_channels_dir, channel_id, info['video_file'])\n",
    "    dst_thumbnail_file = os.path.join(settings.youtube_channels_dir, channel_id, info['thumbnail_file'])\n",
    "    dst_metadata_file = os.path.join(settings.youtube_channels_dir, channel_id, info['metadata_file'])\n",
    "\n",
    "    res = []\n",
    "    if not move_file(src_video_file, dst_video_file):\n",
    "        res.append(src_video_file)\n",
    "    if not move_file(src_thumbnail_file, dst_thumbnail_file):\n",
    "        res.append(src_thumbnail_file)\n",
    "    if move_file(src_metadata_file, dst_metadata_file):\n",
    "        res.append(src_metadata_file)\n",
    "\n",
    "    return res\n",
    "\n",
    "video_ids = list(video_file_index.keys())\n",
    "not_moved = [move_files.remote(x) for x in tqdm(video_ids)]\n",
    "not_moved = [x for x in tqdm(ray.get(not_moved)) if len(x) > 0]\n",
    "\n",
    "with open(os.path.join(settings.data_dir, 'not_moved.json'), 'w') as outfile:\n",
    "    json.dump(not_moved, outfile)\n",
    "\n",
    "print(f'{len(not_moved)} files not moved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # rename file\n",
    "# os.rename(os.path.join(YOUTUBE_VIDEO_DIR, old_file), os.path.join(YOUTUBE_VIDEO_DIR, new_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resolve Channel Ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scan the channel directory for all subdirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_ids = os.listdir(settings.youtube_channels_dir)\n",
    "channel_ids = [x for x in channel_ids if x != '__unknown__']\n",
    "logger.info(f'Found {len(channel_ids)} channels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(settings.data_dir, 'channel_ids.json'), 'w') as outfile:\n",
    "    json.dump(channel_ids, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(settings.data_dir, 'channel_ids.json'), 'r') as infile:\n",
    "    channel_ids = json.load(infile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Request channel info from YouTube for a collection of channel ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f'Requesting info for {len(channel_ids)} channels')\n",
    "\n",
    "# get 50 channels at a time\n",
    "channels_with_meta = []\n",
    "for i in range(0, len(channel_ids), 50):\n",
    "    channels_with_meta += get_channels_from_youtube(channel_ids[i:i+50])\n",
    "\n",
    "logger.info(f'Retrieved info for {len(channels_with_meta)} channels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(settings.data_dir, 'channels_with_meta.json'), 'w') as outfile:\n",
    "    json.dump(channels_with_meta, outfile, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(settings.data_dir, 'channels_with_meta.json'), 'r') as infile:\n",
    "    channels_with_meta = json.load(infile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save_channels(channels_with_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "channels_with_meta_db = get_channels_from_db(channel_ids)\n",
    "\n",
    "channel_meta_ids = set([x['_id'] for x in channels_with_meta_db])\n",
    "\n",
    "missing_ids = [x for x in channel_ids if x not in channel_meta_ids]\n",
    "\n",
    "print(f'{len(missing_ids)} channels missing meta')\n",
    "print(missing_ids)\n",
    "\n",
    "print(len(channels_with_meta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_meta = get_channels_from_youtube(missing_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels_with_meta_index = {}\n",
    "for x in channels_with_meta:\n",
    "    channels_with_meta_index[x['channel_id']] = x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename channel id dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_name(name):\n",
    "    return name.replace('/', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_dirs = os.listdir(settings.youtube_channels_dir)\n",
    "for channel_dir in channel_dirs:\n",
    "    if channel_dir in channels_with_meta_index:\n",
    "        meta = channels_with_meta_index[channel_dir]\n",
    "        title = safe_name(meta[\"channel\"][\"snippet\"][\"title\"])\n",
    "        print(f'{channel_dir} => {title}')\n",
    "        src_dir = os.path.join(settings.youtube_channels_dir, channel_dir)\n",
    "        dst_dir = os.path.join(settings.youtube_channels_dir, title)\n",
    "        if os.path.exists(dst_dir):\n",
    "            shutil.copytree(src_dir, dst_dir, dirs_exist_ok=True)\n",
    "        else:\n",
    "            os.rename(src_dir, dst_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cineplex.videos import download_video\n",
    "\n",
    "try:\n",
    "    res = download_video('https://www.youtube.com/watch?v=BaW_jenozKc')\n",
    "\n",
    "    info = res['info']\n",
    "    id = info['id']\n",
    "    title = info['title']\n",
    "    channel = info['channel']\n",
    "    channel_id = info['channel_id']\n",
    "    video_filename = res['video_filename'] \n",
    "    thumbnail_filename = res['thumbnail_filename']\n",
    "    info_filename = res['info_filename']\n",
    "\n",
    "    print(f'{id=}\\n{title=}\\n{channel=}\\n{channel_id=}\\n{video_filename=}\\n{thumbnail_filename=}\\n{info_filename=}')\n",
    "\n",
    "except Exception as e:\n",
    "    # Logging is already being performed in the download_video function\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # # Get channel details\n",
    "    # request = youtube.channels().list(\n",
    "    #     part=\"snippet,contentDetails,statistics\",\n",
    "    #     mine=True\n",
    "    #     # id=\"UCqsUJL5xIWuidR7sIrPLhAw\",\n",
    "    # )\n",
    "    # request = youtube.search().list(\n",
    "    #     channelId = CHANNEL_ID,\n",
    "    #     part = 'id,snippet',\n",
    "    #     type = 'video',\n",
    "    #     publishedAfter = '2018-12-31T23:59:59Z',\n",
    "    #     publishedBefore = '2020-01-01T00:00:00Z',\n",
    "    #     order = 'date',\n",
    "    #     fields = 'nextPageToken,items(id,snippet)',\n",
    "    #     maxResults = 50\n",
    "    # )\n",
    "    # video_data = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mongo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Migrate from Redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(settings.data_dir, f'playlists_{settings.youtube_my_channel_id}.json'), 'r') as infile:\n",
    "    playlists = json.load(infile)\n",
    "\n",
    "    # remove the channel_id key\n",
    "    playlists['_id'] = playlists['channel_id']\n",
    "    del playlists['channel_id']\n",
    "\n",
    "    get_db().yt_ch_playlists.insert_one(playlists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import redis\n",
    "\n",
    "@ray.remote\n",
    "def migrate_video_info(id):\n",
    "\n",
    "    try:\n",
    "        redis_db = redis.Redis(host=settings.db_host, port=settings.db_port, db=settings.db)\n",
    "        info = redis_db.get(f'video#{id}')\n",
    "        if not info:\n",
    "            logger.error(f'Failed to find {id} in db')\n",
    "            return id\n",
    "        info = json.loads(info)\n",
    "\n",
    "        info['_id'] = info['id']\n",
    "        del info['id']\n",
    "\n",
    "        get_db().yt_videos.insert_one(info)\n",
    "\n",
    "        return None\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f'Failed to migrate video info for {id}: {e}')\n",
    "        return id\n",
    "\n",
    "# from ray.util import inspect_serializability\n",
    "# inspect_serializability(migrate_video_info, name='migrate_video_info')\n",
    "\n",
    "with open(os.path.join(settings.data_dir, 'file_index_videos.json'), 'r') as infile:\n",
    "    video_ids = list(json.load(infile).keys())\n",
    "\n",
    "    not_migrated = [migrate_video_info.remote(id) for id in tqdm(video_ids)]\n",
    "    not_migrated = [x for x in ray.get(not_migrated) if x is not None]\n",
    "\n",
    "    with open(os.path.join(settings.data_dir, 'not_migrated.json'), 'w') as outfile:\n",
    "        json.dump(not_migrated, outfile)\n",
    "\n",
    "    print(f'{len(not_migrated)} files not migrated')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Channel Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cineplex.youtube_channels import (\n",
    "    get_channels_from_youtube,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Channel Playlists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cineplex.youtube_playlists import (\n",
    "    get_channel_playlists_from_db,\n",
    "    get_channel_playlists_from_youtube,\n",
    "    save_channel_playlists,\n",
    "    get_playlist_items_from_db,\n",
    "    get_playlist_items_from_youtube,\n",
    "    save_playlist_items,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = get_db().yt_ch_playlists.find_one({'_id': 'UCqsUJL5xIWuidR7sIrPLhAw'})\n",
    "pprint(res)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ccf20a9f0f41ab70de1e751dc2d6d0f1188277d7e1b89d3e803becd18cd66bba"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

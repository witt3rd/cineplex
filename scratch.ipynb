{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings(my_youtube_channel_id='UCqsUJL5xIWuidR7sIrPLhAw', db='0', db_host='localhost', db_port=9736, mongo_url='mongodb://localhost:27017', mongo_db='cineplex', tmp_dir='./tmp', data_dir='./data', youtube_channels_dir='/Volumes/Cineplex00/YouTube/channels', log_name='cineplex', log_level='DEBUG', log_dir='./logs', log_to_console=True)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "from pprint import pprint\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "import redis\n",
    "import ray\n",
    "from cineplex.db import get_db\n",
    "from cineplex.config import Settings\n",
    "from cineplex.logger import Logger\n",
    "import cineplex.youtube_channels as ytch\n",
    "import cineplex.youtube_playlists as ytpl\n",
    "import cineplex.youtube_videos as ytv\n",
    "import cineplex\n",
    "\n",
    "settings = Settings()\n",
    "logger = Logger()\n",
    "\n",
    "pprint(settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discover Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a recursive list of all files in a directory\n",
    "def get_all_files(dir_path):\n",
    "    all_files = []\n",
    "    for root, dirs, files in os.walk(dir_path):\n",
    "        for file in files:\n",
    "            all_files.append(os.path.join(root, file))\n",
    "    return all_files\n",
    "\n",
    "# files = os.listdir(settings.youtube_channels_dir)\n",
    "files = get_all_files(settings.youtube_channels_dir)\n",
    "print(f'found {len(files)} files in {settings.youtube_channels_dir}')\n",
    "with open('data/channel_files.json', 'w') as outfile:\n",
    "    json.dump(files, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(settings.data_dir, 'channel_files.json'), 'r') as infile:\n",
    "    channel_files = json.load(infile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for root, _, files in os.walk(os.path.join(settings.data_dir, 'yt_playlist_items')):\n",
    "    for file in files:\n",
    "        if not file.startswith('yt_'):\n",
    "            os.rename(os.path.join(root, file), os.path.join(root, f'yt_{file}'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(settings.data_dir, 'bad_metadata.json'), 'r') as infile:\n",
    "    bad_metadata = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_bad_metadata = [x for x in bad_metadata if x]\n",
    "len(new_bad_metadata)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(settings.data_dir, 'bad_metadata.json'), 'w') as outfile:\n",
    "    json.dump(new_bad_metadata, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Move files from channel ID dirs to uploader dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "channels_dir = \"/Volumes/Cineplex00/youtube/channels\"\n",
    "\n",
    "for file in files:\n",
    "    if file.endswith('.json'):\n",
    "        # get dir from file name\n",
    "        dir_name = os.path.dirname(file).split('/')[-1]\n",
    "        full_dir_name = os.path.join(channels_dir, dir_name)\n",
    "\n",
    "        if not os.path.exists(full_dir_name):\n",
    "            continue\n",
    "\n",
    "        if not os.path.exists(file):\n",
    "            continue\n",
    "\n",
    "        with open(file) as json_file:\n",
    "            data = json.load(json_file)\n",
    "\n",
    "        channel_id = data['channel_id'] if 'channel_id' in data else None\n",
    "        if not channel_id:\n",
    "            print(f'{file} does not have a channel_id')\n",
    "            continue\n",
    "\n",
    "        if dir_name != channel_id:\n",
    "            continue\n",
    "\n",
    "        uploader = data['uploader'] if 'uploader' in data else None\n",
    "        if not uploader:\n",
    "            print(f'{file} does not have an uploader')\n",
    "            continue\n",
    "\n",
    "        dst_dir = os.path.join(channels_dir, uploader)\n",
    "        os.makedirs(dst_dir, exist_ok=True)\n",
    "\n",
    "        file_glob = glob.glob(f\"{file[:-9]}*\")\n",
    "        for f in file_glob:\n",
    "            filename = os.path.basename(f)\n",
    "            dst_file = os.path.join(dst_dir, filename)\n",
    "            if os.path.exists(dst_file):\n",
    "                print(f'üóëÔ∏è Deleting duplicate {f}')\n",
    "                os.remove(f)\n",
    "            else:\n",
    "                print(f'üóÇÔ∏è Copying {f} to {dst_dir}')            \n",
    "                shutil.move(f, dst_dir)\n",
    "\n",
    "        if not len(os.listdir(full_dir_name)):\n",
    "            print(f'üóëÔ∏è Removing {full_dir_name}')\n",
    "            shutil.rmtree(full_dir_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/missing_channels.json') as json_file:\n",
    "    missing_channels = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_with_meta_batch = ytch.get_channel_from_youtube_batch(missing_channels)\n",
    "print(f'channel_with_meta_batch: {len(channel_with_meta_batch)}')\n",
    "ytch.save_channel_to_db_batch(channel_with_meta_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dedupe Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_idx = {}\n",
    "\n",
    "with open('data/file_list_videos.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    print(len(data))\n",
    "    for file in data:\n",
    "        filename, ext = os.path.splitext(file)\n",
    "        if filename in filename_idx:\n",
    "            filename_idx[filename].append(ext)\n",
    "        else:\n",
    "            filename_idx[filename] = [ext]\n",
    "\n",
    "dupes = []\n",
    "for filename, extensions in filename_idx.items():\n",
    "    if len(extensions) > 1:\n",
    "        dupes.append(filename)\n",
    "\n",
    "remove = []\n",
    "print(f'found {len(dupes)} duplicate filenames')\n",
    "for dup in dupes:\n",
    "    sizes = {}\n",
    "    for ext in filename_idx[dup]:\n",
    "        size = os.path.getsize(os.path.join(settings.youtube_videos_dir, f'{dup}{ext}'))\n",
    "        sizes[ext] = size\n",
    "\n",
    "    # if len(sizes) > 2:\n",
    "    #     print(f'{dup} has {len(sizes)} copies: {sizes}')\n",
    "\n",
    "    smallest = min(sizes, key=sizes.get)\n",
    "\n",
    "    # print(f'Keeping: {dup}{smallest} @ {sizes[smallest]}')\n",
    "    sizes.pop(smallest)\n",
    "    for ext in sizes:\n",
    "        # print(f'Removing: {dup}{ext} @ {sizes[ext]}')\n",
    "        remove.append(f'{dup}{ext}')\n",
    "\n",
    "print(f'found {len(remove)} files to remove')\n",
    "\n",
    "for file in remove:\n",
    "    os.remove(os.path.join(settings.youtube_videos_dir, file))\n",
    "    data.remove(file)\n",
    "\n",
    "print(len(data))\n",
    "\n",
    "# write the json file\n",
    "with open('data/file_list_videos_deduped.json', 'w') as outfile:\n",
    "    json.dump(data, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragments = set()\n",
    "\n",
    "file_list_videos_clean = []\n",
    "\n",
    "with open(os.path.join(settings.data_dir, 'file_list_videos_deduped.json')) as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "    for file in data:\n",
    "        filename, ext = os.path.splitext(file)\n",
    "\n",
    "        # extract the youtube id from the filename\n",
    "        id = filename[-12:]\n",
    "\n",
    "        # handle fragments\n",
    "        if '.' in id:\n",
    "            filename, ext = os.path.splitext(filename)\n",
    "            id = filename[-11:]\n",
    "            fragments.add(id)\n",
    "\n",
    "        elif id[0] != '-':\n",
    "            print(f'{id}|{filename}')\n",
    "\n",
    "        else:\n",
    "            file_list_videos_clean.append(file)\n",
    "\n",
    "print(f'found {len(fragments)} fragments')\n",
    "print(fragments)\n",
    "\n",
    "with open('data/video_fragments.json', 'w') as outfile:\n",
    "    json.dump(list(fragments), outfile)\n",
    "\n",
    "with open('data/file_list_videos_clean.json', 'w') as outfile:\n",
    "    json.dump(file_list_videos_clean, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video File Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_file_index = {}\n",
    "\n",
    "with open(os.path.join(settings.data_dir, 'file_list_videos_clean.json')) as json_file:\n",
    "    data = json.load(json_file)\n",
    "    for file in data:\n",
    "        filename, ext = os.path.splitext(file)\n",
    "        id = filename[-11:]\n",
    "        video_file_index[id] = {'id': id, 'filename': file}\n",
    "with open(os.path.join(settings.data_dir, 'file_index_videos.json'), 'w') as outfile:\n",
    "    json.dump(video_file_index, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(settings.data_dir, 'file_index_videos.json')) as json_file:\n",
    "    video_file_index = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thumbnail File Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thumbnail_file_index = {}\n",
    "\n",
    "with open(os.path.join(settings.data_dir, 'file_list_thumbnails.json')) as json_file:\n",
    "    data = json.load(json_file)\n",
    "    for file in data:\n",
    "        filename, ext = os.path.splitext(file)\n",
    "        id = filename[-11:]\n",
    "        thumbnail_file_index[id] = {'id': id, 'filename': file}\n",
    "with open(os.path.join(settings.data_dir, 'file_index_thumbnails.json'), 'w') as outfile:\n",
    "    json.dump(thumbnail_file_index, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(settings.data_dir, 'file_index_thumbnails.json')) as json_file:\n",
    "    thumbnail_file_index = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata File Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_file_index = {}\n",
    "\n",
    "with open(os.path.join(settings.data_dir, 'file_list_metadata.json')) as json_file:\n",
    "    data = json.load(json_file)\n",
    "    for file in data:\n",
    "        # metadata files have two extensions\n",
    "        filename, ext = os.path.splitext(file)\n",
    "        filename, ext = os.path.splitext(filename)\n",
    "        id = filename[-11:]\n",
    "        metadata_file_index[id] = {'id': id, 'filename': file}\n",
    "with open(os.path.join(settings.data_dir, 'file_index_metadata.json'), 'w') as outfile:\n",
    "    json.dump(metadata_file_index, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(settings.data_dir, 'file_index_metadata.json')) as json_file:\n",
    "    metadata_file_index = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Thumbnails and Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_thumbnails = []\n",
    "missing_metadata = []\n",
    "\n",
    "for id in video_file_index.keys():\n",
    "    if id not in thumbnail_file_index:\n",
    "        missing_thumbnails.append(id)\n",
    "    if id not in metadata_file_index:\n",
    "        missing_metadata.append(id)\n",
    "\n",
    "print(f'found {len(missing_thumbnails)} missing thumbnails')\n",
    "with open(os.path.join(settings.data_dir, 'missing_thumbnails.json'), 'w') as outfile:\n",
    "    json.dump(missing_thumbnails, outfile)\n",
    "\n",
    "print(f'found {len(missing_metadata)} missing metadata')\n",
    "with open(os.path.join(settings.data_dir, 'missing_metadata.json'), 'w') as outfile:\n",
    "    json.dump(missing_metadata, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract metadata and save to DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "@ray.remote\n",
    "def extract_metadata(info_file):\n",
    "\n",
    "    video_with_meta = ytv.extract_video_info_from_file(info_file)\n",
    "    if video_with_meta is None:\n",
    "        return info_file\n",
    "\n",
    "    ytv.save_video_to_db(video_with_meta, False)\n",
    "    return None\n",
    "\n",
    "info_files = []\n",
    "\n",
    "for x in channel_files:\n",
    "    basename, ext = os.path.splitext(x)\n",
    "    if ext == '.json':\n",
    "        info_files.append(x)\n",
    "\n",
    "print(f'Found {len(info_files)} info files')\n",
    "\n",
    "futures = []\n",
    "for x in tqdm(info_files):\n",
    "    ref = extract_metadata.remote(x)\n",
    "    futures.append(ref)\n",
    "\n",
    "# bad_metadata = [lambda x: extract_metadata.remote(x, files_index[get_basename(x)]) for x in tqdm(info_files[:1])]\n",
    "bad_metadata = [x for x in tqdm(ray.get(futures)) if x]\n",
    "\n",
    "with open(os.path.join(settings.data_dir, 'bad_metadata.json'), 'w') as outfile:\n",
    "    json.dump(bad_metadata, outfile)\n",
    "\n",
    "print(f'Found {len(bad_metadata)} bad metadata')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move Files to Channel Dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def move_file(src, dst):\n",
    "    try:\n",
    "        # print(f'Moving {src} to {dst}')\n",
    "        if not os.path.exists(dst):\n",
    "            os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
    "            shutil.move(src, dst)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f'Failed to move {src} to {dst}: {e}')\n",
    "        return False\n",
    "\n",
    "@ray.remote\n",
    "def move_files(id):\n",
    "\n",
    "    info = get_db().get(f'video#{id}')\n",
    "    if not info:\n",
    "        logger.error(f'Failed to find {id} in db')\n",
    "        return id\n",
    "    info = json.loads(info)\n",
    "\n",
    "    src_video_file = os.path.join(settings.youtube_videos_dir, info['video_file'])\n",
    "    src_thumbnail_file = os.path.join(settings.youtube_thumbnails_dir, info['thumbnail_file'])\n",
    "    src_metadata_file = os.path.join(settings.youtube_metadata_dir, info['metadata_file'])\n",
    "\n",
    "    channel_id = info['channel_id']\n",
    "    if not channel_id:\n",
    "        channel_id = '__unknown__'\n",
    "        \n",
    "    dst_video_file = os.path.join(settings.youtube_channels_dir, channel_id, info['video_file'])\n",
    "    dst_thumbnail_file = os.path.join(settings.youtube_channels_dir, channel_id, info['thumbnail_file'])\n",
    "    dst_metadata_file = os.path.join(settings.youtube_channels_dir, channel_id, info['metadata_file'])\n",
    "\n",
    "    res = []\n",
    "    if not move_file(src_video_file, dst_video_file):\n",
    "        res.append(src_video_file)\n",
    "    if not move_file(src_thumbnail_file, dst_thumbnail_file):\n",
    "        res.append(src_thumbnail_file)\n",
    "    if move_file(src_metadata_file, dst_metadata_file):\n",
    "        res.append(src_metadata_file)\n",
    "\n",
    "    return res\n",
    "\n",
    "video_ids = list(video_file_index.keys())\n",
    "not_moved = [move_files.remote(x) for x in tqdm(video_ids)]\n",
    "not_moved = [x for x in tqdm(ray.get(not_moved)) if len(x) > 0]\n",
    "\n",
    "with open(os.path.join(settings.data_dir, 'not_moved.json'), 'w') as outfile:\n",
    "    json.dump(not_moved, outfile)\n",
    "\n",
    "print(f'{len(not_moved)} files not moved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save cached channel info to DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = []\n",
    "for root, _, files in os.walk(os.path.join(settings.data_dir, 'channels')):\n",
    "    for file in files:\n",
    "        file = os.path.join(root, file)\n",
    "        all_files.append(file)\n",
    "print(f'Found {len(all_files)} files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in all_files:\n",
    "    with open(file, 'r') as infile:\n",
    "        data = json.load(infile)\n",
    "    if 'channel_id' in data:\n",
    "        data['_id'] = data['channel_id']\n",
    "        del data['channel_id']\n",
    "    if '_id' not in data:\n",
    "        logger.error(f'{file} has no channel_id: {data.keys()}')        \n",
    "        continue\n",
    "    save_channel(data, to_disk=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Channel Playlists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_ids = ['UCqsUJL5xIWuidR7sIrPLhAw']\n",
    "channel_with_meta_batch = get_channel_from_youtube_batch(channel_ids)\n",
    "print(channel_with_meta_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_channel_to_db_batch(channel_with_meta_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = get_db().yt_ch_playlists.find_one({'_id': 'UCqsUJL5xIWuidR7sIrPLhAw'})\n",
    "pprint(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resolve Channel Ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scan the channel directory for all subdirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_ids = os.listdir(settings.youtube_channels_dir)\n",
    "channel_ids = [x for x in channel_ids if x != '__unknown__']\n",
    "logger.info(f'Found {len(channel_ids)} channels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(settings.data_dir, 'channel_ids.json'), 'w') as outfile:\n",
    "    json.dump(channel_ids, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(settings.data_dir, 'channel_ids.json'), 'r') as infile:\n",
    "    channel_ids = json.load(infile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Request channel info from YouTube for a collection of channel ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f'Requesting info for {len(channel_ids)} channels')\n",
    "\n",
    "# get 50 channels at a time\n",
    "channels_with_meta = []\n",
    "for i in range(0, len(channel_ids), 50):\n",
    "    channels_with_meta += get_channel_from_youtube_batch(channel_ids[i:i+50])\n",
    "\n",
    "logger.info(f'Retrieved info for {len(channels_with_meta)} channels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(settings.data_dir, 'channels_with_meta.json'), 'w') as outfile:\n",
    "    json.dump(channels_with_meta, outfile, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(settings.data_dir, 'channels_with_meta.json'), 'r') as infile:\n",
    "    channels_with_meta = json.load(infile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save_channels(channels_with_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "channels_with_meta_db = get_channels_from_db(channel_ids)\n",
    "\n",
    "channel_meta_ids = set([x['_id'] for x in channels_with_meta_db])\n",
    "\n",
    "missing_ids = [x for x in channel_ids if x not in channel_meta_ids]\n",
    "\n",
    "print(f'{len(missing_ids)} channels missing meta')\n",
    "print(missing_ids)\n",
    "\n",
    "print(len(channels_with_meta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_meta = get_channels_from_youtube(missing_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels_with_meta_index = {}\n",
    "for x in channels_with_meta:\n",
    "    channels_with_meta_index[x['channel_id']] = x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename channel id dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_name(name):\n",
    "    return name.replace('/', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_dirs = os.listdir(settings.youtube_channels_dir)\n",
    "for channel_dir in channel_dirs:\n",
    "    if channel_dir in channels_with_meta_index:\n",
    "        meta = channels_with_meta_index[channel_dir]\n",
    "        title = safe_name(meta[\"channel\"][\"snippet\"][\"title\"])\n",
    "        print(f'{channel_dir} => {title}')\n",
    "        src_dir = os.path.join(settings.youtube_channels_dir, channel_dir)\n",
    "        dst_dir = os.path.join(settings.youtube_channels_dir, title)\n",
    "        if os.path.exists(dst_dir):\n",
    "            shutil.copytree(src_dir, dst_dir, dirs_exist_ok=True)\n",
    "        else:\n",
    "            os.rename(src_dir, dst_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    res = download_video('https://www.youtube.com/watch?v=BaW_jenozKc')\n",
    "\n",
    "    info = res['info']\n",
    "    id = info['id']\n",
    "    title = info['title']\n",
    "    channel = info['channel']\n",
    "    channel_id = info['channel_id']\n",
    "    video_filename = res['video_filename'] \n",
    "    thumbnail_filename = res['thumbnail_filename']\n",
    "    info_filename = res['info_filename']\n",
    "\n",
    "    print(f'{id=}\\n{title=}\\n{channel=}\\n{channel_id=}\\n{video_filename=}\\n{thumbnail_filename=}\\n{info_filename=}')\n",
    "\n",
    "except Exception as e:\n",
    "    # Logging is already being performed in the download_video function\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # # Get channel details\n",
    "    # request = youtube.channels().list(\n",
    "    #     part=\"snippet,contentDetails,statistics\",\n",
    "    #     mine=True\n",
    "    #     # id=\"UCqsUJL5xIWuidR7sIrPLhAw\",\n",
    "    # )\n",
    "    # request = youtube.search().list(\n",
    "    #     channelId = CHANNEL_ID,\n",
    "    #     part = 'id,snippet',\n",
    "    #     type = 'video',\n",
    "    #     publishedAfter = '2018-12-31T23:59:59Z',\n",
    "    #     publishedBefore = '2020-01-01T00:00:00Z',\n",
    "    #     order = 'date',\n",
    "    #     fields = 'nextPageToken,items(id,snippet)',\n",
    "    #     maxResults = 50\n",
    "    # )\n",
    "    # video_data = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mongo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Migrate from Redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(settings.data_dir, f'playlists_{settings.youtube_my_channel_id}.json'), 'r') as infile:\n",
    "    playlists = json.load(infile)\n",
    "\n",
    "    # remove the channel_id key\n",
    "    playlists['_id'] = playlists['channel_id']\n",
    "    del playlists['channel_id']\n",
    "\n",
    "    get_db().yt_ch_playlists.insert_one(playlists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def migrate_video_info(id):\n",
    "\n",
    "    try:\n",
    "        redis_db = redis.Redis(host=settings.db_host, port=settings.db_port, db=settings.db)\n",
    "        info = redis_db.get(f'video#{id}')\n",
    "        if not info:\n",
    "            logger.error(f'Failed to find {id} in db')\n",
    "            return id\n",
    "        info = json.loads(info)\n",
    "\n",
    "        info['_id'] = info['id']\n",
    "        del info['id']\n",
    "\n",
    "        get_db().yt_videos.insert_one(info)\n",
    "\n",
    "        return None\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f'Failed to migrate video info for {id}: {e}')\n",
    "        return id\n",
    "\n",
    "# from ray.util import inspect_serializability\n",
    "# inspect_serializability(migrate_video_info, name='migrate_video_info')\n",
    "\n",
    "with open(os.path.join(settings.data_dir, 'file_index_videos.json'), 'r') as infile:\n",
    "    video_ids = list(json.load(infile).keys())\n",
    "\n",
    "    not_migrated = [migrate_video_info.remote(id) for id in tqdm(video_ids)]\n",
    "    not_migrated = [x for x in ray.get(not_migrated) if x is not None]\n",
    "\n",
    "    with open(os.path.join(settings.data_dir, 'not_migrated.json'), 'w') as outfile:\n",
    "        json.dump(not_migrated, outfile)\n",
    "\n",
    "    print(f'{len(not_migrated)} files not migrated')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = get_db().yt_videos.find({})\n",
    "\n",
    "videos = list(cursor)\n",
    "\n",
    "skipped = 0\n",
    "for video in videos[:1]:\n",
    "    print(video)\n",
    "    continue\n",
    "    if 'info' in video:\n",
    "        skipped += 1\n",
    "        continue\n",
    "    video_with_meta = {}\n",
    "    video_with_meta['_id'] = video['_id']\n",
    "    video_with_meta['as_of'] = str(datetime.now())\n",
    "    video_with_meta['channel_id'] = video['channel_id']\n",
    "    video['id'] = video['_id']\n",
    "    del video['_id']\n",
    "    video_with_meta['info'] = video\n",
    "\n",
    "    get_db().yt_videos.update_one({'_id': video['id']}, {'$set': video_with_meta}, upsert=True)\n",
    "\n",
    "print(f'{skipped} videos skipped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = {\n",
    "    '_id': 'bar'\n",
    "}\n",
    "\n",
    "print(foo['_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(settings.data_dir, 'offline_playlists.json'), 'r') as infile:\n",
    "    offline_playlists = json.load(infile)\n",
    "\n",
    "with open(os.path.join(settings.data_dir, 'offline_channels.json'), 'r') as infile:\n",
    "    offline_channels = json.load(infile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLIjhfluhNd98r3U5HIlP2yRhlFSJL3v6x üõ°Ô∏è Sparta @ 2021-11-28 13:29:36.512000\n",
      "PLIjhfluhNd9_FkxD3jLfoNwrL_o2Jrbrk üé∞ MDP @ 2021-11-28 13:29:33.156000\n",
      "PLIjhfluhNd980TfK_vipfrX-FQ2GSxxJ3 ‚è∞ Watch Later @ 2021-11-28 13:29:17.899000\n",
      "PLIjhfluhNd9-_wp08TyMG6bikIU-fQyHx ‚ù§Ô∏è Poos @ 2021-11-28 13:29:31.647000\n",
      "PLIjhfluhNd9-AOdTpspxPuprWuOwYKJUH üéß Listen @ 2021-11-28 13:29:32.157000\n",
      "PLIjhfluhNd9_N4DXhFJm3OOwXWiP08bG8 üéÆ Unity @ 2021-11-28 13:29:32.318000\n",
      "PLIjhfluhNd98tnmX4heFW8lrHvz1Ur3FW üíª Code: CPP @ 2021-11-28 13:29:34.072000\n",
      "PLIjhfluhNd9_fICCNgt58UQardoloJ26Z üßòüèª‚Äç‚ôÇÔ∏è Awaken @ 2021-11-28 13:29:36.172000\n",
      "PLIjhfluhNd998KZq6Psf26QyKcDjU0ey1 üïπÔ∏è Video Games @ 2021-11-28 13:29:35.841000\n",
      "PLIjhfluhNd98zSpKsf69arDUm1Dqbe0bn üêç Python @ 2021-11-28 13:29:37.973000\n",
      "PLIjhfluhNd98tYUa6N5sW1ls9Jso6BRJ9 ‚Çø DLT @ 2021-11-28 13:29:36.647000\n",
      "PLIjhfluhNd99eTbybfViftBFgPGjfSMnI üçπ Blender @ 2021-11-28 13:29:31.968000\n",
      "PLIjhfluhNd9_M2RKvuwvrtJxXCBimAILf üíª Code @ 2021-11-28 13:29:33.894000\n",
      "PLIjhfluhNd98KTZg55F736Rv1wkOY0Bei HoTT @ 2021-11-28 13:29:37.290000\n",
      "PLIjhfluhNd9-0Emjp7OejlDRL2CCEfj_Z ‚öñÔ∏è Svelte @ 2021-11-28 13:29:38.255000\n",
      "PLIjhfluhNd9_GQSbeSEli3doJ5PbocoLH üéµ Compose @ 2021-11-28 13:29:33.314000\n",
      "PLIjhfluhNd98NEnMNyTW_Fcnna_1Ijo3F üéÆ Unity: Singletons @ 2021-11-28 13:29:32.915000\n",
      "FLqsUJL5xIWuidR7sIrPLhAw ‚≠ê Favorites @ 2021-11-28 13:29:31.808000\n",
      "PLIjhfluhNd9_9Uvm0aHCuwzHqVHSyw5WX üë©‚Äçüç≥ Pooga Cooks @ 2021-11-28 13:29:33.726000\n",
      "PLIjhfluhNd994focGXggpDJPXMaUhcvPW üíª Code: Functional Programming @ 2021-11-28 13:29:34.584000\n",
      "PLIjhfluhNd98FnN888FaUJWJ8spj7xKHA üéµ Compose: Ableton @ 2021-11-28 13:29:33.584000\n",
      "PLIjhfluhNd99U7LDLtXtKJ808ktE-y5P8 ‚å®Ô∏è Emacs @ 2021-11-28 13:28:52.443000\n",
      "PLIjhfluhNd9_2zVd12FYi9NLCmIAYupyo üîß Machining @ 2021-11-28 13:29:37.574000\n",
      "PLIjhfluhNd9-oPNZlggzBetaOxxfJZ9x2 üñ®Ô∏è 3D Printing @ 2021-11-28 13:29:36.914000\n",
      "PLIjhfluhNd99bZLmXxDJSH5yomEOeLIxs üíª Code: Idris @ 2021-11-28 13:29:34.745000\n",
      "PLIjhfluhNd9-SBcsZtt7Vl1q-0faAr9_5 üîä YT Music @ 2021-11-28 13:29:35.329000\n",
      "PLIjhfluhNd9_EZ5NaYyWfa4tBygp8oSfR üî´ Ray @ 2021-11-28 13:29:35.492000\n",
      "PLIjhfluhNd9-AHVCaCSXJUB1ZurI_Piad üéÆ Unity: Ragdoll @ 2021-11-28 13:29:32.602000\n",
      "PLIjhfluhNd9_RIzDA-S4QdlGySRJ-ueK_ Physics @ 2021-11-28 13:29:37.812000\n",
      "PLIjhfluhNd98RchSS5AZr9K-6IoqyfnFV üìö Books @ 2021-11-28 13:29:34.970000\n",
      "PLIjhfluhNd9_pBzNY5JE5BOu7K24IGThd üéÆ Unity: DOTS @ 2021-11-28 13:29:32.458000\n",
      "PLIjhfluhNd98kW7E2osazwZle7WiTY4sp ‚ö° Electronics @ 2021-11-28 13:29:37.132000\n",
      "PLIjhfluhNd99ldXA4Ca0gpzTtivEUuAhd üéÆ Unity: Scriptable Objects @ 2021-11-28 13:29:32.734000\n",
      "PLIjhfluhNd99ea8ystO8fGDuV0GqIeD_Q üöÄ Paradox Shift @ 2021-11-28 13:29:36.311000\n",
      "PLIjhfluhNd98fqUOc_i1tl9zm7YNKg_A9 ü§ñ DL @ 2021-11-28 13:29:36.036000\n",
      "PLIjhfluhNd98LSyU6E_z8Z4NsswJuDBxH üìö Books: House of Leaves @ 2021-11-28 13:29:35.091000\n",
      "PLIjhfluhNd9-Mlh9W19Tf3w99zle4zmDA ü§ñ Robotics @ 2021-11-28 13:29:38.092000\n",
      "PLIjhfluhNd99sC8ZLF86uJG6-o27mxMTl üïµÔ∏è‚Äç‚ôÇÔ∏è ex nihilo @ 2021-11-28 13:29:35.615000\n",
      "PLIjhfluhNd9-YduUjMW8HeHTviwWJX7D0 üíª Code: DevOps @ 2021-11-28 13:29:34.420000\n",
      "PLIjhfluhNd98L-pVh99mk3IyVDeHN3lwE üíª Code: Data-Oriented @ 2021-11-28 13:29:34.298000\n",
      "PLIjhfluhNd992Y5rZbXpA33c6LL3gBGqU üì∫ Video Ideas @ 2021-11-28 13:29:38.576000\n",
      "PLIjhfluhNd98R5Heiaxr8An14ltOvtXML üìΩ Movies @ 2021-11-28 13:29:35.184000\n",
      "PLIjhfluhNd9_pYPwpSJOswdh8mcdsu7EE üïπÔ∏è Video Games: RPG Math @ 2021-11-28 13:29:35.933000\n",
      "PLeXS0cAkuTPqFMtZQ379qdEmcfxO1SvXc The Bible in a Year (with Fr. Mike Schmitz) @ 2021-11-28 13:29:38.471000\n"
     ]
    }
   ],
   "source": [
    "x = list(ytpl.get_offline_playlists_from_db())\n",
    "for y in x:\n",
    "    print(f\"{y['_id']} {y['playlist']['snippet']['title']} @ {y['offline_as_of']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UCB6PV0cvJpzlcXRG7nz6PpQ Motherboard @ 2021-11-28 13:29:55.304000\n",
      "UCm325cMiw9B15xl22_gr6Dw Beau Miles @ 2021-11-28 13:29:48.975000\n",
      "UC-tLyAaPbRZiYrOJxAGB7dQ Pursuit of Wonder @ 2021-11-28 13:29:57.221000\n",
      "UChturLXwYxwTOf_5krs0qvA element14 presents @ 2021-11-28 13:29:51.728000\n",
      "UCDUBDASXQ-rVXsjqblQwG7A DTRHRadioArchives @ 2021-11-28 13:29:51.459000\n",
      "UCS0N5baNlQWJCUrhCEo8WlA Ben Eater @ 2021-11-28 13:29:49.146000\n",
      "UCZvKxeDiXk31KspCQO3EC6g TomCAT - Characters, Art and Tutorials @ 2021-11-28 13:29:59.772000\n",
      "UCEbYhDd6c6vngsF5PQpFVWg Tsoding @ 2021-11-28 13:30:00.198000\n",
      "UCU7OLO4-Vzxo1aW7mAuI3lg SovereignDev @ 2021-11-28 13:29:58.966000\n",
      "UCYO_jab_esuFRV4b17AJtAw 3Blue1Brown @ 2021-11-28 13:29:47.027000\n",
      "UCRS4DvO9X7qaqVYUW2_dwOw Rock the JVM @ 2021-11-28 13:29:57.732000\n",
      "UCQrD9U5-jVdmzF2Bt7cjtNg Ned Makes Games @ 2021-11-28 13:29:55.583000\n",
      "UCzgkOWKcwy0uhYilE6bd1Lg Zaiste Programming @ 2021-11-28 13:30:02.478000\n",
      "UCmGSJVG3mCRXVOP4yZrU1Dw Johnny Harris @ 2021-11-28 13:29:54.168000\n",
      "UCZFipeZtQM5CKUjx6grh54g Isaac Arthur @ 2021-11-28 13:29:53.068000\n",
      "UC6-ymYjG0SU0jUWnWh9ZzEQ Wisecrack @ 2021-11-28 13:30:01.933000\n",
      "UC7qPftDWPw9XuExpSgfkmJQ Nostalgia Nerd @ 2021-11-28 13:29:56.354000\n",
      "UCeIg_PnAoyd1w6y8BelLdiQ Ziverge @ 2021-11-28 13:30:02.638000\n",
      "UCz6X8QK9_JG49hJxnzAu-1w Samaneri JayasƒÅra - Wisdom of the Masters @ 2021-11-28 13:29:57.914000\n",
      "UCsXVk37bltHxD1rDPwtNM8Q Kurzgesagt ‚Äì In a Nutshell @ 2021-11-28 13:29:54.581000\n",
      "UClXK00EtgGK2n1hhgqDqvDQ Alan Neachell @ 2021-11-28 13:29:47.752000\n",
      "UCswumVYAf0SKczPSoRGHxcQ Worakls @ 2021-11-28 13:30:02.306000\n",
      "UCzzw01KOI4jbE0lWM2FdGJw DroiD @ 2021-11-28 13:29:51.139000\n",
      "UCUbDcUPed50Y_7KmfCXKohA James Bruton @ 2021-11-28 13:29:53.237000\n",
      "UCSdQo_Kbd-AKr8Re0P-1z6g jj rieger @ 2021-11-28 13:29:53.923000\n",
      "UC1D3yD4wlPMico0dss264XA NileBlue @ 2021-11-28 13:29:55.754000\n",
      "UCivA7_KLKWo43tFcCkFvydw Applied Science @ 2021-11-28 13:29:48.447000\n",
      "UCFK6NCbuCIVzA6Yj1G_ZqCg Code Monkey @ 2021-11-28 13:29:50.177000\n",
      "UCrqM0Ym_NbK1fqeQG2VIohg Tsoding Daily @ 2021-11-28 13:30:00.381000\n",
      "UCWvq4kcdNI1r1jZKFw9TiUA ScienceClic English @ 2021-11-28 13:29:58.189000\n",
      "UCjFqcJQXGZ6T6sxyFB-5i6A Every Frame a Painting @ 2021-11-28 13:29:51.909000\n",
      "UCmtyQOKKmrMVaKuRXz02jbQ Sebastian Lague @ 2021-11-28 13:29:58.355000\n",
      "UClsFdM0HzTdF1JYoraQ0aUw Brick Experiment Channel @ 2021-11-28 13:29:49.749000\n",
      "UCyoayn_uVt2I55ZCUuBVRcQ Info Gamer @ 2021-11-28 13:29:52.874000\n",
      "UCc-N24Y5OA0gqbjBwe1ttfA Weird History @ 2021-11-28 13:30:01.068000\n",
      "UCC-UOdK8-mIjxBQm_ot1T-Q Cracking The Cryptic @ 2021-11-28 13:29:50.703000\n",
      "UCKzJFdi57J53Vr_BkTfN3uQ Primer @ 2021-11-28 13:29:56.955000\n",
      "UCL_f53ZEJxp8TtlOkHwMV9Q Jordan B Peterson @ 2021-11-28 13:29:54.346000\n",
      "UC1Kxtc6DOexi4JT-t57Ey9g Applied Category Theory @ 2021-11-28 13:29:48.152000\n",
      "UCmOwsoHty5PrmE-3QhUBfPQ Jay Alammar @ 2021-11-28 13:29:53.419000\n",
      "UCrk8Y2fsR5i_5c1iTR9tZpg Uberboyo @ 2021-11-28 13:30:00.552000\n",
      "UCgBncpylJ1kiVaPyP-PZauQ Serrano.Academy @ 2021-11-28 13:29:54.763000\n",
      "UCosnWgi3eorc1klEQ8pIgJQ Afrotechmods @ 2021-11-28 13:29:47.471000\n",
      "UC9-y-6csu5WGm29I7JiwpnA Computerphile @ 2021-11-28 13:29:50.534000\n",
      "UC4QZ_LsYcvcq7qOsOhpAX4A ColdFusion @ 2021-11-28 13:29:50.361000\n",
      "UCsnGwSIHyoYN0kiINAGUKxg Wolfgang's Channel @ 2021-11-28 13:30:02.122000\n",
      "UC0BtDzepe0odmpRbmpi5q7Q Favonia @ 2021-11-28 13:29:52.110000\n",
      "UC6JhS4GvWf3AJfOTfkrse2w Simulation @ 2021-11-28 13:29:58.535000\n",
      "UCzfWju7SFoWLCyV_gDVCrGA Imphenzia @ 2021-11-28 13:29:52.621000\n",
      "UC3xdLFFsqG701QAyGJIPT1g Philipp Hagenlocher @ 2021-11-28 13:29:56.717000\n",
      "UCgeicB5AuF3MyyUto0-M5Lw Actualized.org @ 2021-11-28 13:29:47.211000\n",
      "UC2v1PAoSTALNwHbWPkqYYNw Matrix Explained @ 2021-11-28 13:29:54.942000\n",
      "UCy1f4m64dwCwk8CBZ_vHfPg CGMatter @ 2021-11-28 13:29:49.921000\n",
      "UCfMJ2MchTSW2kWaT0kK94Yw William Osman @ 2021-11-28 13:30:01.549000\n",
      "UCtHaxi4GTYDpJgMSGy7AeSw Michael Reeves @ 2021-11-28 13:29:55.123000\n",
      "UCvXRQt1lFboY1XK4ERGzQIQ Jed McKenna @ 2021-11-28 13:29:53.666000\n",
      "UCOKHwx1VCdgnxwbjyb9Iu1g Blender Guru @ 2021-11-28 13:29:49.578000\n",
      "UCxUZjZaLPsHPlp8AtkSBZiA Topos Institute @ 2021-11-28 13:30:00.033000\n",
      "UCJ0kmBes7vSjT3QoWDfjwwg The New American Video @ 2021-11-28 13:29:59.508000\n",
      "UCvZe6ZCbF9xgbbbdkiodPKQ Baumgartner Restoration @ 2021-11-28 13:29:48.798000\n",
      "UCveB47lgzZJ1WOf4XYVJNBw Beyond the press @ 2021-11-28 13:29:49.406000\n",
      "UCZPFjMe1uRSirmSpznqvJfQ Valerio Velardo - The Sound of AI @ 2021-11-28 13:30:00.812000\n",
      "UCC5vubsnFBpy-lFB8S8rZEg Anni Vuohensilta @ 2021-11-28 13:29:47.941000\n",
      "UC0fDG3byEcMtbOqPMymDNbw Noclip - Video Game Documentaries @ 2021-11-28 13:29:56.108000\n",
      "UCcMDMoNu66_1Hwi5-MeiQgw Hydraulic Press Channel @ 2021-11-28 13:29:52.371000\n",
      "UCDETFHKteb-C_EaXmRKvP4w Sisyphus 55 @ 2021-11-28 13:29:58.707000\n",
      "UC8uT9cgJorJPWu7ITLGo9Ww The 8-Bit Guy @ 2021-11-28 13:29:59.328000\n",
      "UCsYYksPHiGqXHPoHI-fm5sg Whiteboard Crypto @ 2021-11-28 13:30:01.304000\n",
      "UC1JTQBa5QxZCpXrFSkMxmPw Raycevick @ 2021-11-28 13:29:57.491000\n",
      "UCpMcsdZf2KkAnfmxiq2MfMQ Arvin Ash @ 2021-11-28 13:29:48.622000\n",
      "UCFhXFikryT4aFcLkLw2LBLA NileRed @ 2021-11-28 13:29:55.933000\n",
      "UCAiiOTio8Yu69c3XnR7nQBQ System Crafters @ 2021-11-28 13:29:59.150000\n",
      "UCbguawtJlHjxXzdAskubQVg William Osman 2 @ 2021-11-28 13:30:01.733000\n",
      "UCdpWKLNfbROyoGPV46-zaUQ Default Cube @ 2021-11-28 13:29:50.951000\n",
      "UC7yF9tV4xWEMZkel7q8La_w PeterSripol @ 2021-11-28 13:29:56.520000\n"
     ]
    }
   ],
   "source": [
    "x = ytch.get_offline_channels_from_db()\n",
    "for y in x:\n",
    "    print(f\"{y['_id']} {y['channel']['snippet']['title']} @ {y['offline_as_of']}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ccf20a9f0f41ab70de1e751dc2d6d0f1188277d7e1b89d3e803becd18cd66bba"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

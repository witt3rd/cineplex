{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings(my_youtube_channel_id='UCqsUJL5xIWuidR7sIrPLhAw', db='0', db_host='localhost', db_port=9736, mongo_url='mongodb://localhost:27017', mongo_db='cineplex', tmp_dir='./tmp', data_dir='./data', youtube_channels_dir='/Volumes/Cineplex00/YouTube/channels', log_name='cineplex', log_level='DEBUG', log_dir='./logs', log_to_console=True)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "from pprint import pprint\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "import redis\n",
    "import ray\n",
    "from cineplex.db import get_db\n",
    "from cineplex.config import Settings\n",
    "from cineplex.logger import Logger\n",
    "import cineplex.youtube_channels as ytch\n",
    "import cineplex.youtube_playlists as ytpl\n",
    "import cineplex.youtube_videos as ytv\n",
    "\n",
    "settings = Settings()\n",
    "logger = Logger()\n",
    "\n",
    "pprint(settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discover Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a recursive list of all files in a directory\n",
    "def get_all_files(dir_path):\n",
    "    all_files = []\n",
    "    for root, dirs, files in os.walk(dir_path):\n",
    "        for file in files:\n",
    "            all_files.append(os.path.join(root, file))\n",
    "    return all_files\n",
    "\n",
    "# files = os.listdir(settings.youtube_channels_dir)\n",
    "files = get_all_files(settings.youtube_channels_dir)\n",
    "print(f'found {len(files)} files in {settings.youtube_channels_dir}')\n",
    "with open('data/channel_files.json', 'w') as outfile:\n",
    "    json.dump(files, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(settings.data_dir, 'channel_files.json'), 'r') as infile:\n",
    "    channel_files = json.load(infile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for root, _, files in os.walk(os.path.join(settings.data_dir, 'yt_playlist_items')):\n",
    "    for file in files:\n",
    "        if not file.startswith('yt_'):\n",
    "            os.rename(os.path.join(root, file), os.path.join(root, f'yt_{file}'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(settings.data_dir, 'bad_metadata.json'), 'r') as infile:\n",
    "    bad_metadata = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_bad_metadata = [x for x in bad_metadata if x]\n",
    "len(new_bad_metadata)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(settings.data_dir, 'bad_metadata.json'), 'w') as outfile:\n",
    "    json.dump(new_bad_metadata, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Move files from channel ID dirs to uploader dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "channels_dir = \"/Volumes/Cineplex00/youtube/channels\"\n",
    "\n",
    "for file in files:\n",
    "    if file.endswith('.json'):\n",
    "        # get dir from file name\n",
    "        dir_name = os.path.dirname(file).split('/')[-1]\n",
    "        full_dir_name = os.path.join(channels_dir, dir_name)\n",
    "\n",
    "        if not os.path.exists(full_dir_name):\n",
    "            continue\n",
    "\n",
    "        if not os.path.exists(file):\n",
    "            continue\n",
    "\n",
    "        with open(file) as json_file:\n",
    "            data = json.load(json_file)\n",
    "\n",
    "        channel_id = data['channel_id'] if 'channel_id' in data else None\n",
    "        if not channel_id:\n",
    "            print(f'{file} does not have a channel_id')\n",
    "            continue\n",
    "\n",
    "        if dir_name != channel_id:\n",
    "            continue\n",
    "\n",
    "        uploader = data['uploader'] if 'uploader' in data else None\n",
    "        if not uploader:\n",
    "            print(f'{file} does not have an uploader')\n",
    "            continue\n",
    "\n",
    "        dst_dir = os.path.join(channels_dir, uploader)\n",
    "        os.makedirs(dst_dir, exist_ok=True)\n",
    "\n",
    "        file_glob = glob.glob(f\"{file[:-9]}*\")\n",
    "        for f in file_glob:\n",
    "            filename = os.path.basename(f)\n",
    "            dst_file = os.path.join(dst_dir, filename)\n",
    "            if os.path.exists(dst_file):\n",
    "                print(f'ðŸ—‘ï¸ Deleting duplicate {f}')\n",
    "                os.remove(f)\n",
    "            else:\n",
    "                print(f'ðŸ—‚ï¸ Copying {f} to {dst_dir}')            \n",
    "                shutil.move(f, dst_dir)\n",
    "\n",
    "        if not len(os.listdir(full_dir_name)):\n",
    "            print(f'ðŸ—‘ï¸ Removing {full_dir_name}')\n",
    "            shutil.rmtree(full_dir_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/missing_channels.json') as json_file:\n",
    "    missing_channels = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_with_meta_batch = ytch.get_channel_from_youtube_batch(missing_channels)\n",
    "print(f'channel_with_meta_batch: {len(channel_with_meta_batch)}')\n",
    "ytch.save_channel_to_db_batch(channel_with_meta_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dedupe Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_idx = {}\n",
    "\n",
    "with open('data/file_list_videos.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    print(len(data))\n",
    "    for file in data:\n",
    "        filename, ext = os.path.splitext(file)\n",
    "        if filename in filename_idx:\n",
    "            filename_idx[filename].append(ext)\n",
    "        else:\n",
    "            filename_idx[filename] = [ext]\n",
    "\n",
    "dupes = []\n",
    "for filename, extensions in filename_idx.items():\n",
    "    if len(extensions) > 1:\n",
    "        dupes.append(filename)\n",
    "\n",
    "remove = []\n",
    "print(f'found {len(dupes)} duplicate filenames')\n",
    "for dup in dupes:\n",
    "    sizes = {}\n",
    "    for ext in filename_idx[dup]:\n",
    "        size = os.path.getsize(os.path.join(settings.youtube_videos_dir, f'{dup}{ext}'))\n",
    "        sizes[ext] = size\n",
    "\n",
    "    # if len(sizes) > 2:\n",
    "    #     print(f'{dup} has {len(sizes)} copies: {sizes}')\n",
    "\n",
    "    smallest = min(sizes, key=sizes.get)\n",
    "\n",
    "    # print(f'Keeping: {dup}{smallest} @ {sizes[smallest]}')\n",
    "    sizes.pop(smallest)\n",
    "    for ext in sizes:\n",
    "        # print(f'Removing: {dup}{ext} @ {sizes[ext]}')\n",
    "        remove.append(f'{dup}{ext}')\n",
    "\n",
    "print(f'found {len(remove)} files to remove')\n",
    "\n",
    "for file in remove:\n",
    "    os.remove(os.path.join(settings.youtube_videos_dir, file))\n",
    "    data.remove(file)\n",
    "\n",
    "print(len(data))\n",
    "\n",
    "# write the json file\n",
    "with open('data/file_list_videos_deduped.json', 'w') as outfile:\n",
    "    json.dump(data, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragments = set()\n",
    "\n",
    "file_list_videos_clean = []\n",
    "\n",
    "with open(os.path.join(settings.data_dir, 'file_list_videos_deduped.json')) as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "    for file in data:\n",
    "        filename, ext = os.path.splitext(file)\n",
    "\n",
    "        # extract the youtube id from the filename\n",
    "        id = filename[-12:]\n",
    "\n",
    "        # handle fragments\n",
    "        if '.' in id:\n",
    "            filename, ext = os.path.splitext(filename)\n",
    "            id = filename[-11:]\n",
    "            fragments.add(id)\n",
    "\n",
    "        elif id[0] != '-':\n",
    "            print(f'{id}|{filename}')\n",
    "\n",
    "        else:\n",
    "            file_list_videos_clean.append(file)\n",
    "\n",
    "print(f'found {len(fragments)} fragments')\n",
    "print(fragments)\n",
    "\n",
    "with open('data/video_fragments.json', 'w') as outfile:\n",
    "    json.dump(list(fragments), outfile)\n",
    "\n",
    "with open('data/file_list_videos_clean.json', 'w') as outfile:\n",
    "    json.dump(file_list_videos_clean, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video File Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_file_index = {}\n",
    "\n",
    "with open(os.path.join(settings.data_dir, 'file_list_videos_clean.json')) as json_file:\n",
    "    data = json.load(json_file)\n",
    "    for file in data:\n",
    "        filename, ext = os.path.splitext(file)\n",
    "        id = filename[-11:]\n",
    "        video_file_index[id] = {'id': id, 'filename': file}\n",
    "with open(os.path.join(settings.data_dir, 'file_index_videos.json'), 'w') as outfile:\n",
    "    json.dump(video_file_index, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(settings.data_dir, 'file_index_videos.json')) as json_file:\n",
    "    video_file_index = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thumbnail File Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thumbnail_file_index = {}\n",
    "\n",
    "with open(os.path.join(settings.data_dir, 'file_list_thumbnails.json')) as json_file:\n",
    "    data = json.load(json_file)\n",
    "    for file in data:\n",
    "        filename, ext = os.path.splitext(file)\n",
    "        id = filename[-11:]\n",
    "        thumbnail_file_index[id] = {'id': id, 'filename': file}\n",
    "with open(os.path.join(settings.data_dir, 'file_index_thumbnails.json'), 'w') as outfile:\n",
    "    json.dump(thumbnail_file_index, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(settings.data_dir, 'file_index_thumbnails.json')) as json_file:\n",
    "    thumbnail_file_index = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata File Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_file_index = {}\n",
    "\n",
    "with open(os.path.join(settings.data_dir, 'file_list_metadata.json')) as json_file:\n",
    "    data = json.load(json_file)\n",
    "    for file in data:\n",
    "        # metadata files have two extensions\n",
    "        filename, ext = os.path.splitext(file)\n",
    "        filename, ext = os.path.splitext(filename)\n",
    "        id = filename[-11:]\n",
    "        metadata_file_index[id] = {'id': id, 'filename': file}\n",
    "with open(os.path.join(settings.data_dir, 'file_index_metadata.json'), 'w') as outfile:\n",
    "    json.dump(metadata_file_index, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(settings.data_dir, 'file_index_metadata.json')) as json_file:\n",
    "    metadata_file_index = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Thumbnails and Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_thumbnails = []\n",
    "missing_metadata = []\n",
    "\n",
    "for id in video_file_index.keys():\n",
    "    if id not in thumbnail_file_index:\n",
    "        missing_thumbnails.append(id)\n",
    "    if id not in metadata_file_index:\n",
    "        missing_metadata.append(id)\n",
    "\n",
    "print(f'found {len(missing_thumbnails)} missing thumbnails')\n",
    "with open(os.path.join(settings.data_dir, 'missing_thumbnails.json'), 'w') as outfile:\n",
    "    json.dump(missing_thumbnails, outfile)\n",
    "\n",
    "print(f'found {len(missing_metadata)} missing metadata')\n",
    "with open(os.path.join(settings.data_dir, 'missing_metadata.json'), 'w') as outfile:\n",
    "    json.dump(missing_metadata, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract metadata and save to DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "@ray.remote\n",
    "def extract_metadata(info_file):\n",
    "\n",
    "    video_with_meta = ytv.extract_video_info_from_file(info_file)\n",
    "    if video_with_meta is None:\n",
    "        return info_file\n",
    "\n",
    "    ytv.save_video_to_db(video_with_meta, False)\n",
    "    return None\n",
    "\n",
    "info_files = []\n",
    "\n",
    "for x in channel_files:\n",
    "    basename, ext = os.path.splitext(x)\n",
    "    if ext == '.json':\n",
    "        info_files.append(x)\n",
    "\n",
    "print(f'Found {len(info_files)} info files')\n",
    "\n",
    "futures = []\n",
    "for x in tqdm(info_files):\n",
    "    ref = extract_metadata.remote(x)\n",
    "    futures.append(ref)\n",
    "\n",
    "# bad_metadata = [lambda x: extract_metadata.remote(x, files_index[get_basename(x)]) for x in tqdm(info_files[:1])]\n",
    "bad_metadata = [x for x in tqdm(ray.get(futures)) if x]\n",
    "\n",
    "with open(os.path.join(settings.data_dir, 'bad_metadata.json'), 'w') as outfile:\n",
    "    json.dump(bad_metadata, outfile)\n",
    "\n",
    "print(f'Found {len(bad_metadata)} bad metadata')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move Files to Channel Dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def move_file(src, dst):\n",
    "    try:\n",
    "        # print(f'Moving {src} to {dst}')\n",
    "        if not os.path.exists(dst):\n",
    "            os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
    "            shutil.move(src, dst)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f'Failed to move {src} to {dst}: {e}')\n",
    "        return False\n",
    "\n",
    "@ray.remote\n",
    "def move_files(id):\n",
    "\n",
    "    info = get_db().get(f'video#{id}')\n",
    "    if not info:\n",
    "        logger.error(f'Failed to find {id} in db')\n",
    "        return id\n",
    "    info = json.loads(info)\n",
    "\n",
    "    src_video_file = os.path.join(settings.youtube_videos_dir, info['video_file'])\n",
    "    src_thumbnail_file = os.path.join(settings.youtube_thumbnails_dir, info['thumbnail_file'])\n",
    "    src_metadata_file = os.path.join(settings.youtube_metadata_dir, info['metadata_file'])\n",
    "\n",
    "    channel_id = info['channel_id']\n",
    "    if not channel_id:\n",
    "        channel_id = '__unknown__'\n",
    "        \n",
    "    dst_video_file = os.path.join(settings.youtube_channels_dir, channel_id, info['video_file'])\n",
    "    dst_thumbnail_file = os.path.join(settings.youtube_channels_dir, channel_id, info['thumbnail_file'])\n",
    "    dst_metadata_file = os.path.join(settings.youtube_channels_dir, channel_id, info['metadata_file'])\n",
    "\n",
    "    res = []\n",
    "    if not move_file(src_video_file, dst_video_file):\n",
    "        res.append(src_video_file)\n",
    "    if not move_file(src_thumbnail_file, dst_thumbnail_file):\n",
    "        res.append(src_thumbnail_file)\n",
    "    if move_file(src_metadata_file, dst_metadata_file):\n",
    "        res.append(src_metadata_file)\n",
    "\n",
    "    return res\n",
    "\n",
    "video_ids = list(video_file_index.keys())\n",
    "not_moved = [move_files.remote(x) for x in tqdm(video_ids)]\n",
    "not_moved = [x for x in tqdm(ray.get(not_moved)) if len(x) > 0]\n",
    "\n",
    "with open(os.path.join(settings.data_dir, 'not_moved.json'), 'w') as outfile:\n",
    "    json.dump(not_moved, outfile)\n",
    "\n",
    "print(f'{len(not_moved)} files not moved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save cached channel info to DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = []\n",
    "for root, _, files in os.walk(os.path.join(settings.data_dir, 'channels')):\n",
    "    for file in files:\n",
    "        file = os.path.join(root, file)\n",
    "        all_files.append(file)\n",
    "print(f'Found {len(all_files)} files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in all_files:\n",
    "    with open(file, 'r') as infile:\n",
    "        data = json.load(infile)\n",
    "    if 'channel_id' in data:\n",
    "        data['_id'] = data['channel_id']\n",
    "        del data['channel_id']\n",
    "    if '_id' not in data:\n",
    "        logger.error(f'{file} has no channel_id: {data.keys()}')        \n",
    "        continue\n",
    "    save_channel(data, to_disk=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Channel Playlists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_ids = ['UCqsUJL5xIWuidR7sIrPLhAw']\n",
    "channel_with_meta_batch = get_channel_from_youtube_batch(channel_ids)\n",
    "print(channel_with_meta_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_channel_to_db_batch(channel_with_meta_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = get_db().yt_ch_playlists.find_one({'_id': 'UCqsUJL5xIWuidR7sIrPLhAw'})\n",
    "pprint(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resolve Channel Ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scan the channel directory for all subdirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_ids = os.listdir(settings.youtube_channels_dir)\n",
    "channel_ids = [x for x in channel_ids if x != '__unknown__']\n",
    "logger.info(f'Found {len(channel_ids)} channels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(settings.data_dir, 'channel_ids.json'), 'w') as outfile:\n",
    "    json.dump(channel_ids, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(settings.data_dir, 'channel_ids.json'), 'r') as infile:\n",
    "    channel_ids = json.load(infile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Request channel info from YouTube for a collection of channel ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f'Requesting info for {len(channel_ids)} channels')\n",
    "\n",
    "# get 50 channels at a time\n",
    "channels_with_meta = []\n",
    "for i in range(0, len(channel_ids), 50):\n",
    "    channels_with_meta += get_channel_from_youtube_batch(channel_ids[i:i+50])\n",
    "\n",
    "logger.info(f'Retrieved info for {len(channels_with_meta)} channels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(settings.data_dir, 'channels_with_meta.json'), 'w') as outfile:\n",
    "    json.dump(channels_with_meta, outfile, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(settings.data_dir, 'channels_with_meta.json'), 'r') as infile:\n",
    "    channels_with_meta = json.load(infile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save_channels(channels_with_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "channels_with_meta_db = get_channels_from_db(channel_ids)\n",
    "\n",
    "channel_meta_ids = set([x['_id'] for x in channels_with_meta_db])\n",
    "\n",
    "missing_ids = [x for x in channel_ids if x not in channel_meta_ids]\n",
    "\n",
    "print(f'{len(missing_ids)} channels missing meta')\n",
    "print(missing_ids)\n",
    "\n",
    "print(len(channels_with_meta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_meta = get_channels_from_youtube(missing_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels_with_meta_index = {}\n",
    "for x in channels_with_meta:\n",
    "    channels_with_meta_index[x['channel_id']] = x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename channel id dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_name(name):\n",
    "    return name.replace('/', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_dirs = os.listdir(settings.youtube_channels_dir)\n",
    "for channel_dir in channel_dirs:\n",
    "    if channel_dir in channels_with_meta_index:\n",
    "        meta = channels_with_meta_index[channel_dir]\n",
    "        title = safe_name(meta[\"channel\"][\"snippet\"][\"title\"])\n",
    "        print(f'{channel_dir} => {title}')\n",
    "        src_dir = os.path.join(settings.youtube_channels_dir, channel_dir)\n",
    "        dst_dir = os.path.join(settings.youtube_channels_dir, title)\n",
    "        if os.path.exists(dst_dir):\n",
    "            shutil.copytree(src_dir, dst_dir, dirs_exist_ok=True)\n",
    "        else:\n",
    "            os.rename(src_dir, dst_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    res = download_video('https://www.youtube.com/watch?v=BaW_jenozKc')\n",
    "\n",
    "    info = res['info']\n",
    "    id = info['id']\n",
    "    title = info['title']\n",
    "    channel = info['channel']\n",
    "    channel_id = info['channel_id']\n",
    "    video_filename = res['video_filename'] \n",
    "    thumbnail_filename = res['thumbnail_filename']\n",
    "    info_filename = res['info_filename']\n",
    "\n",
    "    print(f'{id=}\\n{title=}\\n{channel=}\\n{channel_id=}\\n{video_filename=}\\n{thumbnail_filename=}\\n{info_filename=}')\n",
    "\n",
    "except Exception as e:\n",
    "    # Logging is already being performed in the download_video function\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # # Get channel details\n",
    "    # request = youtube.channels().list(\n",
    "    #     part=\"snippet,contentDetails,statistics\",\n",
    "    #     mine=True\n",
    "    #     # id=\"UCqsUJL5xIWuidR7sIrPLhAw\",\n",
    "    # )\n",
    "    # request = youtube.search().list(\n",
    "    #     channelId = CHANNEL_ID,\n",
    "    #     part = 'id,snippet',\n",
    "    #     type = 'video',\n",
    "    #     publishedAfter = '2018-12-31T23:59:59Z',\n",
    "    #     publishedBefore = '2020-01-01T00:00:00Z',\n",
    "    #     order = 'date',\n",
    "    #     fields = 'nextPageToken,items(id,snippet)',\n",
    "    #     maxResults = 50\n",
    "    # )\n",
    "    # video_data = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mongo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Migrate from Redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(settings.data_dir, f'playlists_{settings.youtube_my_channel_id}.json'), 'r') as infile:\n",
    "    playlists = json.load(infile)\n",
    "\n",
    "    # remove the channel_id key\n",
    "    playlists['_id'] = playlists['channel_id']\n",
    "    del playlists['channel_id']\n",
    "\n",
    "    get_db().yt_ch_playlists.insert_one(playlists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def migrate_video_info(id):\n",
    "\n",
    "    try:\n",
    "        redis_db = redis.Redis(host=settings.db_host, port=settings.db_port, db=settings.db)\n",
    "        info = redis_db.get(f'video#{id}')\n",
    "        if not info:\n",
    "            logger.error(f'Failed to find {id} in db')\n",
    "            return id\n",
    "        info = json.loads(info)\n",
    "\n",
    "        info['_id'] = info['id']\n",
    "        del info['id']\n",
    "\n",
    "        get_db().yt_videos.insert_one(info)\n",
    "\n",
    "        return None\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f'Failed to migrate video info for {id}: {e}')\n",
    "        return id\n",
    "\n",
    "# from ray.util import inspect_serializability\n",
    "# inspect_serializability(migrate_video_info, name='migrate_video_info')\n",
    "\n",
    "with open(os.path.join(settings.data_dir, 'file_index_videos.json'), 'r') as infile:\n",
    "    video_ids = list(json.load(infile).keys())\n",
    "\n",
    "    not_migrated = [migrate_video_info.remote(id) for id in tqdm(video_ids)]\n",
    "    not_migrated = [x for x in ray.get(not_migrated) if x is not None]\n",
    "\n",
    "    with open(os.path.join(settings.data_dir, 'not_migrated.json'), 'w') as outfile:\n",
    "        json.dump(not_migrated, outfile)\n",
    "\n",
    "    print(f'{len(not_migrated)} files not migrated')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = get_db().yt_videos.find({})\n",
    "\n",
    "videos = list(cursor)\n",
    "\n",
    "skipped = 0\n",
    "for video in videos[:1]:\n",
    "    print(video)\n",
    "    continue\n",
    "    if 'info' in video:\n",
    "        skipped += 1\n",
    "        continue\n",
    "    video_with_meta = {}\n",
    "    video_with_meta['_id'] = video['_id']\n",
    "    video_with_meta['as_of'] = str(datetime.now())\n",
    "    video_with_meta['channel_id'] = video['channel_id']\n",
    "    video['id'] = video['_id']\n",
    "    del video['_id']\n",
    "    video_with_meta['info'] = video\n",
    "\n",
    "    get_db().yt_videos.update_one({'_id': video['id']}, {'$set': video_with_meta}, upsert=True)\n",
    "\n",
    "print(f'{skipped} videos skipped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = {\n",
    "    '_id': 'bar'\n",
    "}\n",
    "\n",
    "print(foo['_id'])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ccf20a9f0f41ab70de1e751dc2d6d0f1188277d7e1b89d3e803becd18cd66bba"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
